这个仓库包含了一个基于检索增强生成（RAG）技术的本地聊天机器人应用程序的代码，支持多种大语言模型（LLM），便于用户配置自己选择的模型。以下是对仓库内容的详细介绍：

### 整体功能概述
该应用程序是一个完整的智能助手应用，涵盖前端页面、后端服务、样式设置、日志处理、控制台显示以及打包脚本等功能。

### 文件结构与功能
#### 前端文件
- **`frontend/index.html`**：
    - **功能**：定义智能助手的 HTML 结构，包括聊天界面和配置模态框。
    - **关键元素**：聊天容器、输入框、发送按钮用于用户与助手交互；配置按钮点击后打开配置模态框，模态框包含“模型”和“知识库”两个选项卡，用于设置模型参数和管理知识库文档。
- **`frontend/scripts.js`**：
    - **功能**：实现前端交互逻辑，如发送消息、管理配置模态框、处理文档上传和删除等。
    - **关键函数**：`sendMessage()` 发送用户输入的问题到后端并显示助手的响应；`openConfigModal()` 和 `closeConfigModal()` 打开和关闭配置模态框；`addDocuments()` 和 `deleteSelectedFile()` 添加和删除知识库文档；`saveKnowledgeBase()` 将知识库文档和系统提示保存到后端。
- **`frontend/styles.css`**：
    - **功能**：定义应用程序的样式，包括聊天界面、配置模态框、按钮等的外观。
    - **关键样式**：聊天容器和消息框的样式，区分用户和助手消息；配置模态框的样式，包括选项卡、下拉框和保存按钮；文档列表和操作按钮的样式，如添加、删除和保存按钮。

#### 后端文件
- **`backend/http_server.py`**：
    - **功能**：使用 FastAPI 框架实现后端 API，处理模型配置、文档上传和检索等请求。
    - **关键路由**：`GET /api/models` 获取支持和选定模型的配置信息；`PUT /api/models` 将模型配置信息保存到 `dyn_config.toml` 文件；`GET /api/documents` 获取知识库文档列表和系统提示；`POST /api/upload-documents` 和 `POST /api/documents` 上传文档并保存系统提示到后端；`GET /api/config-suspense` 检查是否有挂起的配置更改。
- **`backend/console_window.py`**：
    - **功能**：实现一个自定义控制台窗口，用于显示日志信息。
    - **关键类和方法**：`CustomConsole` 自定义控制台窗口类，包含文本编辑框和切换按钮；`append_text()` 向控制台窗口添加文本信息；`toggle_visibility()` 切换控制台窗口的可见性。
- **`backend/logging_config.py`**：
    - **功能**：提供日志服务的单例实体。

#### 配置文件
- **`backend/sta_config.toml`**：静态配置文件，定义了各种大语言模型提供商的基本信息，如 Ollama、OpenAI、MoonShot 等的基础 URL、支持的 LLM 模型、嵌入模型和简介等，同时设置了默认的 LLM 提供商和模型、嵌入提供商和模型。
- **`backend/factory.toml`**：工厂配置文件，包含知识库和部署的默认配置，如默认的知识库文档和机器人描述，以及默认的 LLM 提供商、模型和嵌入提供商、模型。
- **`backend/dyn_config.toml`**：动态配置文件，可在运行时更新，存储用户对模型和知识库的配置更改。

#### 打包脚本
- **`buildexe.bat`**：
    - **功能**：使用 PyInstaller 将后端脚本打包成可执行文件，并将相关文件复制到打包目录。
    - **关键步骤**：检查 PyInstaller 是否安装；生成版本信息文件；使用 PyInstaller 打包后端脚本；将配置文件、前端文件、资源文件和文档复制到打包目录。
- **`innoSetup.iss`**：
    - **功能**：一个安装程序脚本，最终创建一个单一的安装程序 `unisetup.exe`。
    - **工具**：Inno Setup Compiler。

### 其他文件
- **`README.md`**：项目的英文说明文档，介绍项目的基本信息、各文件的功能等。
- **`README_cn.md`**：项目的中文说明文档。
- **`终端用户手册.md`**：为终端用户提供使用说明，包括如何配置模型和知识库等操作步骤。
- **`developer_guide.md`**：为开发者提供开发指南。
- **`environment.yaml`**：定义了项目所需的 Conda 环境依赖。
- **`requirements.txt`**：定义了项目所需的 Python 包依赖。
- **`reset_config.py`**：可能用于重置配置的脚本。
- **`.gitignore`**：指定 Git 版本控制中需要忽略的文件和目录。
- **`LICENSE`**：项目的许可证文件，该项目采用 MIT 许可证。

### 依赖管理
从 `environment.yaml` 文件可以看出，项目依赖众多 Python 库，包括用于网络请求的 `aiohttp`、`httpx`，用于大语言模型集成的 `langchain` 相关库，用于深度学习的 `pytorch` 相关库，以及用于界面开发的 `PyQt5` 等。同时，还使用了 `conda` 来管理环境。

### 主要功能模块
- **模型配置**：支持多种大语言模型提供商，用户可以在配置窗口中选择不同的 LLM 提供商和模型，以及嵌入向量提供商和模型。
- **知识库管理**：用户可以查看和管理知识库文档，上传和删除文档，系统会根据知识库内容回答用户问题。
- **日志显示**：通过自定义控制台窗口显示日志信息，方便用户监控系统运行状态。
- **聊天交互**：用户可以在聊天界面输入问题，系统会调用相应的大语言模型进行回答，并将回答显示在聊天界面。

### 代码中的关键类和函数
- **`backend/rag_service.py` 中的 `RagService` 类**：
    - **功能**：实现基于检索增强生成（RAG）的聊天服务，包括文档嵌入、历史对话管理、问题回答等功能。
    - **关键方法**：`setup_service()` 初始化服务，包括加载文档、创建向量数据库、构建对话链等；`__ask__()` 处理用户问题，调用对话链进行回答，并将回答拆分为摘要和推理过程。
- **`backend/rag_service.py` 中的 `ModelConfig` 类**：
    - **功能**：负责模型的实例化和文档的读取，根据配置信息创建相应的大语言模型和嵌入模型实例。
    - **关键方法**：`instantiate_llm()` 根据 LLM 提供商和模型名称实例化大语言模型；`instantiate_emb()` 根据嵌入提供商和模型名称实例化嵌入模型；`read_documents()` 读取知识库文档并进行处理。
- **`backend/uni_config.py` 中的 `UniConfig` 类**：
    - **功能**：负责配置文件的加载和更新，管理静态配置、动态配置和工厂配置。
    - **关键方法**：`load_config()` 加载配置文件；`update_knowledge_base()` 更新知识库配置；`reload_config()` 重新加载配置。

### 系统架构
该系统采用前后端分离的架构，前端负责用户界面的展示和交互，后端负责处理业务逻辑和数据存储。前端通过调用后端的 API 进行数据交互，后端使用 FastAPI 框架提供 RESTful API 服务。同时，系统使用 RAG 技术，将知识库文档进行嵌入处理，存储在向量数据库中，在回答用户问题时，通过检索知识库中的相关文档来增强回答的准确性。

### 部署和使用
用户可以通过以下步骤部署和使用该应用程序：
1. 创建 Conda 环境：根据 `environment.yaml` 文件创建项目所需的 Conda 环境。
2. 启动后端服务：运行 `backend/http_server.py` 启动后端服务。
3. 打开前端页面：在浏览器中打开 `frontend/index.html` 访问应用程序。
4. 配置模型和知识库：点击聊天界面右上角的配置按钮，在配置窗口中选择模型和管理知识库文档。
5. 开始聊天：在聊天界面输入问题，系统会给出回答。

### 总结
这个仓库提供了一个完整的基于 RAG 技术的本地聊天机器人应用程序的实现，具有丰富的功能和良好的可配置性，适合需要搭建本地智能助手的用户和开发者使用。通过对不同大语言模型的支持和知识库的管理，用户可以根据自己的需求定制聊天机器人的功能。同时，项目的代码结构清晰，文档详细，便于开发者进行二次开发和扩展。 